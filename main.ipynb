{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# import Settings\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_topics = [\"RAG\", \"Agent\"]\n",
    "num_results_per_topic = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = \"\"\"\n",
    "# Research Paper Report on RAG - Retrieval Augmented Generation and Agentic World.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Retrieval Augmented Generation (RAG) and Agents\n",
    "2.1. Fundamentals of RAG and Agents.\n",
    "2.2. Current State and Applications\n",
    "\n",
    "## 3. Latest Papers:\n",
    "3.1. HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings\n",
    "3.2. MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems\n",
    "3.3. VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding\n",
    "\n",
    "## 4. Conclusion:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./research_results/{\"_\".join(research_paper_topics)}'\n",
    "os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def download_papers(client, topics, num_results_per_topic, directory):\n",
    "    \"\"\"Function to download papers from arxiv for given topics and number of results per topic\"\"\"\n",
    "    for topic in topics:\n",
    "\n",
    "        # sort by recent data and with max results\n",
    "        search = arxiv.Search(\n",
    "            query = topic,\n",
    "            max_results = num_results_per_topic,\n",
    "            sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        # get the results\n",
    "        results = client.results(search)\n",
    "\n",
    "        # download the pdf\n",
    "        save_dir = directory+\"/pdfs\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for r in results:\n",
    "            r.download_pdf(dirpath=save_dir)\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    # List all .pdf files using pathlib\n",
    "\n",
    "    pdf_files = [file.name for file in Path(directory+\"/pdfs\").glob('*.pdf')]\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client\n",
    "client = arxiv.Client()\n",
    "\n",
    "download_papers(client, research_paper_topics, num_results_per_topic, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "def parse_files(directory, pdf_files):\n",
    "    \"\"\"Function to parse the pdf files using LlamaParse in markdown format\"\"\"\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "        num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for index, pdf_file in enumerate(pdf_files):\n",
    "        print(f\"Processing file {index + 1}/{len(pdf_files)}: {pdf_file}\")\n",
    "        document = parser.load_data(f\"{directory}/pdfs/{pdf_file}\")\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_files = list_pdf_files(directory)\n",
    "print(\">>>>> found files: \", pdf_files)\n",
    "\n",
    "documents = parse_files(directory, pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Output containing the authors names, authors companies, and general AI tags.\"\"\"\n",
    "\n",
    "    author_names: List[str] = Field(..., description=\"List of author names of the paper. Give empty list if not available\")\n",
    "\n",
    "    author_companies: List[str] = Field(..., description=\"List of author companies of the paper. Give empty list if not available\")\n",
    "\n",
    "    ai_tags: List[str] = Field(..., description=\"List of general AI tags related to the paper. Give empty list if not available\")\n",
    "\n",
    "\n",
    "async def get_papers_metadata(text, llm):\n",
    "    \"\"\"Function to get the metadata from the given paper\"\"\"\n",
    "    prompt_template = PromptTemplate(\"\"\"Generate authors names, authors companies, and general top 3 AI tags for the given research paper.\n",
    "\n",
    "    Research Paper:\n",
    "\n",
    "    {text}\"\"\")\n",
    "\n",
    "    # https://docs.llamaindex.ai/en/stable/api_reference/llms/#llama_index.core.llms.llm.LLM.astructured_predict\n",
    "    metadata = await llm.astructured_predict(\n",
    "        output_cls=Metadata,\n",
    "        prompt=prompt_template,\n",
    "        text=text,\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "async def get_document_with_metadata(document, llm):\n",
    "    \"\"\"Function to create a Document object with metadata\"\"\"\n",
    "    text_for_metadata_extraction = document[0].text + document[1].text + document[2].text\n",
    "    full_text = \"\\n\\n\".join([doc.text for doc in document])\n",
    "    metadata = await get_papers_metadata(text_for_metadata_extraction, llm)\n",
    "    \n",
    "    return Document(\n",
    "        text=full_text,\n",
    "        metadata={\n",
    "            'author_names': metadata.author_names,\n",
    "            'author_companies': metadata.author_companies,\n",
    "            'ai_tags': metadata.ai_tags\n",
    "        }\n",
    "    )\n",
    "                 \n",
    "async def create_index(documents, llm, embed_model):\n",
    "    \"\"\"Function to create a local vector store index\"\"\"\n",
    "    # Create Document objects with metadata\n",
    "    extract_jobs = []\n",
    "    for document in documents:\n",
    "        extract_jobs.append(get_document_with_metadata(document, llm))\n",
    "    \n",
    "    document_objects = await run_jobs(extract_jobs, workers=4)\n",
    "\n",
    "    # Create semantic node parser for more intelligent text splitting\n",
    "    # embed_model = OpenAIEmbedding()\n",
    "    node_parser = SemanticSplitterNodeParser(\n",
    "        buffer_size=1, \n",
    "        breakpoint_percentile_threshold=95,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # Create and return the index\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=document_objects,\n",
    "        node_parser=node_parser,\n",
    "        show_progress=True  # Added to show indexing progress\n",
    "    )\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = await create_index(documents, llm, embed_model)\n",
    "\n",
    "# Persist the index to disk\n",
    "index.storage_context.persist(directory+\"/storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, you can load the index from disk\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=directory+\"/storage\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    dense_similarity_top_k=10,\n",
    "    sparse_similarity_top_k=10,\n",
    "    alpha=0.5,\n",
    "    enable_reranking=True,\n",
    "    rerank_top_n = 5,\n",
    "    retrieval_mode=\"chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_title(outline):\n",
    "    \"\"\"Function to extract the title from the first line of the outline\"\"\"\n",
    "\n",
    "    first_line = outline.strip().split('\\n')[0]\n",
    "    return first_line.strip('# ').strip()\n",
    "\n",
    "def generate_query_with_llm(title, section, subsection, llm):\n",
    "    \"\"\"Function to generate a query for a report using LLM\"\"\"\n",
    "\n",
    "    prompt = f\"Generate a research query for a report on {title}. \"\n",
    "    prompt += f\"The query should be for the subsection '{subsection}' under the main section '{section}'. \"\n",
    "    prompt += \"The query should guide the research to gather relevant information for this part of the report. The query should be clear, short and concise. \"\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    return str(response).strip()\n",
    "\n",
    "def classify_query(query, llm):\n",
    "    \"\"\"Function to classify the query as either 'LLM' or 'INDEX' based on the query content\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Classify the following query as either \"LLM\" if it can be answered directly by a large language model with general knowledge, or \"INDEX\" if it likely requires querying an external index or database for specific or up-to-date information.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Consider the following:\n",
    "    1. If the query asks for general knowledge, concepts, or explanations, classify as \"LLM\".\n",
    "    2. If the query asks for specific facts, recent events, or detailed information that might not be in the LLM's training data, classify as \"INDEX\".\n",
    "    3. If unsure, err on the side of \"INDEX\".\n",
    "\n",
    "    Classification:\"\"\"\n",
    "\n",
    "    classification = str(llm.complete(prompt)).strip().upper()\n",
    "\n",
    "    if classification not in [\"LLM\", \"INDEX\"]:\n",
    "        print(\"Classification is not clear, defaulting to INDEX\")\n",
    "        classification = \"INDEX\"  # Default to INDEX if the response is unclear\n",
    "\n",
    "    return classification\n",
    "\n",
    "def parse_outline_and_generate_queries(outline, llm):\n",
    "    \"\"\"Function to parse the outline and generate queries for each section and subsection\"\"\"\n",
    "    \n",
    "    lines = outline.strip().split('\\n')\n",
    "    title = extract_title(outline)\n",
    "    current_section = \"\"\n",
    "    queries = {}\n",
    "\n",
    "    for line in lines[1:]:  # Skip the title line\n",
    "        if line.startswith('## '): # This is used in the code to identify section headers in the outline format, where sections are numbered like \"2. Retrieval Augmented Generation (RAG) and Agents\" or \"3. Latest Papers\".\n",
    "            current_section = line.strip('# ').strip()\n",
    "            queries[current_section] = {}\n",
    "        elif re.match(r'^\\d+\\.\\d+\\.', line): # This is used in the code to identify subsection headers in the outline format, where sections are numbered like \"2.1. Fundamentals of RAG\" or \"3.2. Current Applications\".\n",
    "            subsection = line.strip()\n",
    "            query = generate_query_with_llm(title, current_section, subsection, llm)\n",
    "            classification = classify_query(query, llm)\n",
    "            queries[current_section][subsection] = {\"query\": query, \"classification\": classification}\n",
    "\n",
    "    # Handle sections without subsections\n",
    "    for section in queries:\n",
    "        if not queries[section]:\n",
    "            query = generate_query_with_llm(title, section, \"General overview\", llm)\n",
    "            queries[section][\"General\"] = {\"query\": query, \"classification\": \"LLM\"}\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, Context, step\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ReportGenerationEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_engine: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.query_engine = query_engine\n",
    "        self.llm = llm or OpenAI(model='gpt-4o-mini')\n",
    "\n",
    "    def format_report(self, section_contents, outline):\n",
    "        \"\"\"Format the report based on the section contents.\"\"\"\n",
    "        report = \"\"\n",
    "\n",
    "        for section, subsections in section_contents.items():\n",
    "            section_match = re.match(r'^(\\d+\\.)\\s*(.*)$', section)\n",
    "            if section_match:\n",
    "                section_num, section_title = section_match.groups()\n",
    "                \n",
    "                if \"introduction\" in section.lower():\n",
    "                    introduction_num, introduction_title = section_num, section_title\n",
    "                elif \"conclusion\" in section.lower():\n",
    "                    conclusion_num, conclusion_title = section_num, section_title\n",
    "                else:\n",
    "                    combined_content = \"\\n\".join(subsections.values())\n",
    "                    summary_query = f\"Provide a short summary for section '{section}':\\n\\n{combined_content}\"\n",
    "                    section_summary = str(llm.complete(summary_query))\n",
    "                    report += f\"# {section_num} {section_title}\\n\\n{section_summary}\\n\\n\"\n",
    "\n",
    "                    report = self.get_subsections_content(subsections, report)\n",
    "\n",
    "        # Add introduction\n",
    "        introduction_query = f\"Create an introduction for the report:\\n\\n{report}\"\n",
    "        introduction = str(self.llm.complete(introduction_query))\n",
    "        report = f\"# {introduction_num} {introduction_title}\\n\\n{introduction}\\n\\n\" + report\n",
    "\n",
    "        # Add conclusion\n",
    "        conclusion_query = f\"Create a conclusion for the report:\\n\\n{report}\"\n",
    "        conclusion = str(self.llm.complete(conclusion_query))\n",
    "        report += f\"# {conclusion_num} {conclusion_title}\\n\\n{conclusion}\"\n",
    "\n",
    "        # Add title\n",
    "        title = extract_title(outline)\n",
    "        report = f\"# {title}\\n\\n{report}\"\n",
    "        return report\n",
    "\n",
    "    def get_subsections_content(self, subsections, report):\n",
    "        \"\"\"Generate content for each subsection in the outline.\"\"\"\n",
    "        # Sort subsections by their keys before adding them to the report\n",
    "        for subsection in sorted(subsections.keys(), key=lambda x: re.search(r'(\\d+\\.\\d+)', x).group(1) if re.search(r'(\\d+\\.\\d+)', x) else x):\n",
    "            content = subsections[subsection]\n",
    "            subsection_match = re.search(r'(\\d+\\.\\d+)\\.\\s*(.+)', subsection)\n",
    "            if subsection_match:\n",
    "                subsection_num, subsection_title = subsection_match.groups()\n",
    "                report += f\"## {subsection_num} {subsection_title}\\n\\n{content}\\n\\n\"\n",
    "            else:\n",
    "                report += f\"## {subsection}\\n\\n{content}\\n\\n\"\n",
    "        return report\n",
    "\n",
    "    def generate_section_content(self, queries, reverse=False):\n",
    "        \"\"\"Generate content for each section and subsection in the outline.\"\"\"\n",
    "        section_contents = {}\n",
    "        for section, subsections in queries.items():\n",
    "            section_contents[section] = {}\n",
    "            subsection_keys = reversed(sorted(subsections.keys())) if reverse else sorted(subsections.keys())\n",
    "            for subsection in subsection_keys:\n",
    "                data = subsections[subsection]\n",
    "                query = data['query']\n",
    "                classification = data['classification']\n",
    "                if classification == \"LLM\":\n",
    "                    answer = str(llm.complete(query + \" Give a short answer.\"))\n",
    "                else:\n",
    "                    answer = str(query_engine.query(query))\n",
    "                section_contents[section][subsection] = answer\n",
    "        return section_contents\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def queries_generation_event(self, ctx: Context, ev: StartEvent) -> ReportGenerationEvent:\n",
    "        \"\"\"Generate queries for the report.\"\"\"\n",
    "        ctx.data[\"outline\"] = ev.outline\n",
    "        queries = parse_outline_and_generate_queries(ctx.data[\"outline\"], self.llm)\n",
    "\n",
    "        return ReportGenerationEvent(queries=queries)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(\n",
    "        self, ctx: Context, ev: ReportGenerationEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Generate report.\"\"\"\n",
    "\n",
    "        queries = ev.queries\n",
    "\n",
    "        # Generate contents for sections in reverse order\n",
    "        section_contents = self.generate_section_content(queries, reverse=True)\n",
    "        # Format and compile the final report\n",
    "        report = self.format_report(section_contents, ctx.data[\"outline\"])\n",
    "       \n",
    "        return StopEvent(result={\"response\": report})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "draw_all_possible_flows(ReportGenerationAgent, filename=\"report_generation_agent.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReportGenerationAgent(\n",
    "    query_engine=query_engine,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    timeout=1200.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = await agent.run(outline=outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_most_recent_execution(agent, filename=\"report_generation_agent_execution.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + \"/report.md\", \"w\") as f:\n",
    "    f.write(report['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
